x = diag(rnorm(100, mean = 0.02, sd = 2), 100, 100)
det(x)
n = 100
r1 = det(x)^{-n}
print(r1)
det(x)
det(x)
x = diag(rnorm(100, mean = 1, sd = 2), 100, 100)
det(x)
det()
r1 = det(x)^{-n}
print(r1)
x = diag(rnorm(20, mean = 1, sd = 2), 100, 100)
n = 20
x = diag(rnorm(20, mean = 1, sd = 2), 100, 100)
det(x)
det()
r1 = det(x)^{-n}
print(r1)
n = 20
x = diag(rnorm(20, mean = 10, sd = 2), 100, 100)
det(x)
det()
r1 = det(x)^{-n}
print(r1)
n = 200
x = diag(rnorm(200, mean = 10, sd = 2), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 200
x = diag(rnorm(200, mean = 1, sd = 2), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 200
x = diag(rnorm(200, mean = 1, sd = 1), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 200
x = diag(rnorm(200, mean = 1, sd = 0.5), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 200
x = diag(rnorm(200, mean = 0.2, sd = 0.5), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 200
x = diag(rnorm(200, mean = 0, sd = 1), 100, 100)
r1 = det(x)^{-n}
print(r1)
x
n = 20
x = diag(rnorm(20, mean = 0, sd = 1), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 20
x = diag(rnorm(100, mean = 0, sd = 1), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 20
x = diag(rnorm(100, mean = 5, sd = 1), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 100
x = diag(rnorm(100, mean = 5, sd = 1), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 100
x = diag(rnorm(100, mean = 5, sd = 10), 100, 100)
r1 = det(x)^{-n}
print(r1)
x = diag(rnorm(100, mean = 50, sd = 10), 100, 100)
n = 100
x = diag(rnorm(100, mean = 50, sd = 10), 100, 100)
r1 = det(x)^{-n}
print(r1)
n = 100
x = diag(rnorm(100, mean = 0.02, sd = 2), 100, 100)
det(x)
det()
r1 = det(x)^{-n}
print(r1)
n = 100
x = diag(rnorm(100, mean = 0.02, sd = 2), 100, 100)
r1 = det(x)^{-n}
print(r1)
det(x)
n = 10
x = diag(rnorm(10, mean = 0.02, sd = 2), 10, 10)
r1 = det(x)^{-n}
print(r1)
r2 = det(x%*%x)
r2 = det(x%*%x)^{-n/2}
print(r2)
exp(0.21)
rnorm(n = 1000, mean = 70, sd = 10)
x = rnorm(n = 1000, mean = 70, sd = 10)
plot(density(x))
summary(X)
summary(x)
install.packages("tidyverse")
setwd("C:/Users/Portatil/Desktop/Natura/271025_Results_Med/")
# Cargar librerías
library(readxl)
library(tidyverse)
library(MASS)
library(caret)
library(gmodels)
library(mvnormtest)
# Cargar base de datos
dataset = readxl::read_excel("output\\clean_med_dataset_27102025.xlsx")
N = nrow(dataset)
# Cargar diccionario
diccionario_clasificado <- read_excel("output/diccionario_med.xlsx")
# Seleccionar variables del Módulo 1
cat_m1 <- c("edad_r2", "pais", "p3_agregado", "p5_agregado",
"p7_agregado", "p8_agregado", "p9_estrato3", "p40")
cont_m1 <- c("p1edad")
# Seleccionar variables del Módulo 2
cat_m2 <- c("edad_r2", "pais",
"p13","p14",
"p15_autos_agregado",
"p15_1_autos_propios_agregado",
"p16_motos_agregado",
"p16_1_motos_propias_agregado",
"p17_modo_agregado",
"cilindraje_auto_agregado",
"cilindraje_moto_agregado",
"modelo_vehiculo_agregado",
"p19comuna", "p22",
"p23_agregado")
cont_m2 <- c("p1edad", "p18",
"p18_p1",
"p18_p2",
"p18_p3",
"p18_p4",
"p18_c1")
# Seleccionar variables del Módulo 3
cat_m3 <- c(
#"p25_razones_agregadas",       # Razones para elección de modo
"p26_agregado",                # Aspecto que menos le gusta
#"p27_situaciones_multiples",   # Situaciones percibidas / evitadas
"p29_modo_ideal_agregado",     # Modo de transporte ideal
"p30_razon_no_uso_agregado",   # Razones para no usar modo ideal
"p31_fuente_contaminacion_agregada", # Fuente percibida de contaminación
"p33_modo_contaminante_agregado",    # Modo más contaminante percibido
"p35_razon_agregada"                # Razones principales (agregadas)
)
cont_m3 <- c(
"p24",
"p28_importancia_costo_compra",
"p28_importancia_costo_uso",
"p28_importancia_comodidad",
"p28_importancia_tiempo",
"p28_importancia_riesgo_robo",
"p28_importancia_riesgo_acoso",
"p28_importancia_discriminacion",
"p28_importancia_emisiones",
"p28_importancia_siniestralidad",
"p32_contaminacion_likert",
"p36_influencia_amigos",
"p37_influencia_familia"
)
# Seleccionar variables del Módulo 4
cat_m4 <- c("p38p38_1", "p38p38_2", "p38p38_3",
"p38p38_4","p38p38_5","p38p38_6","p38p38_7","p38p38_99"
)
# Vector de variables categóricas
cat_vars = c(cat_m1, cat_m2, cat_m3, cat_m4)
# Vector de variables continuas
cont_vars = c(cont_m1, cont_m2, cont_m3)
#----------------------------#
# Crear variables previas    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#----------------------------#
dataset$id = dataset$cuestionario
dataset$medio = dataset$p17_modo_agregado
dataset = dataset %>% dplyr::select(id, medio, cat_vars, cont_vars)
# Variable de tiempo
dataset$tiempo_total <- rowSums(
dataset[, c("p18", "p18_p1", "p18_p2", "p18_p3", "p18_p4", "p18_c1")],
na.rm = TRUE
)
dataset = dataset %>% dplyr::select(-c("p18", "p18_p1", "p18_p2", "p18_p3", "p18_p4", "p18_c1"))
# Variables continuas
cont_vars = cont_vars[!cont_vars %in%c("p18", "p18_p1", "p18_p2", "p18_p3", "p18_p4", "p18_c1")]
dataset <- dataset %>%
dplyr::mutate(
across(all_of(cat_vars), ~ as.factor(.x)),    # Categóricas a factor
across(all_of(cont_vars), ~ as.numeric(.x))   # Continuas a numérico
)
# Tabla de resumen NAs
na_summary <- dataset %>%
summarise(across(everything(),
~ sum(is.na(.)),
.names = "na_{.col}")) %>%
tidyr::pivot_longer(everything(),
names_to = "variable",
values_to = "n_missing") %>%
mutate(prop_missing = round(n_missing / nrow(dataset) * 100, 2))
na_summary %>%
arrange(desc(prop_missing)) %>%
head(20)   # muestra las 20 con más NA
# Gráfica de missings
library(ggplot2)
na_summary %>%
filter(n_missing > 0) %>%
ggplot(aes(x = reorder(variable, prop_missing),
y = prop_missing)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(x = "Variable",
y = "% de valores faltantes",
title = "Proporción de valores NA por variable") +
theme_minimal(base_size = 13)
#-------------------------------#
# Guardar dataset: input.famd   #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#-------------------------------#
writexl::write_xlsx(dataset, "output/input_famd_med_29102025.xlsx")
# Librerías
library(tidyverse)
# Definir directorio
setwd("C:/Users/Portatil/Desktop/Natura/271025_Results_Med/")
# Cargar base de datos
dataset = readxl::read_excel("output/input_famd_med_29102025.xlsx")
dataset <- dataset %>%
dplyr::mutate(
across(all_of(cat_vars), ~ as.factor(.x)),    # Categóricas a factor
across(all_of(cont_vars), ~ as.numeric(.x))   # Continuas a numérico
)  %>%
dplyr::select(-c("cilindraje_camion_agregado"))
dataset <- dataset %>%
dplyr::mutate(
across(all_of(cat_vars), ~ as.factor(.x)),    # Categóricas a factor
across(all_of(cont_vars), ~ as.numeric(.x))   # Continuas a numérico
)
# Creación de la función para la creación de la matriz
corr_f <- function(df){
# crear output
cor_out <- matrix(NA, nrow = length(colnames(df)), ncol = length(colnames(df)))
colnames(cor_out) <- colnames(df); rownames(cor_out) <- colnames(df)
for (i in colnames(df)) {
for (j in colnames(df)) {
print(paste0(i, " vs ", j))
df.aux <- df %>% dplyr::select(i,j)
if (ncol(df.aux) ==  1) {
df.aux <- cbind(df.aux, df.aux)
colnames(df.aux) <- c("x","y")
} else{colnames(df.aux) <- c("x","y")}
nrow <- which(rownames(cor_out) == i)
ncol <- which(colnames(cor_out) == j)
if (i == j & class(df.aux$x) != "numeric") {
cor_out[nrow,ncol] = length(levels(as.factor(df.aux$x))) - 1
}
if (i == j & class(df.aux$x) == "numeric") {
cor_out[nrow,ncol] = 1
}
if (i != j & (class(df.aux$x) == "numeric" & class(df.aux$y) == "numeric")) {
cor_out[nrow,ncol] = cor(df.aux$x, df.aux$y)
cor_out[ncol, nrow] =  cor(df.aux$x, df.aux$y)
}
library(sjstats)
if (i != j & (class(df.aux$x) != "numeric" & class(df.aux$y) == "numeric")) {
cor_out[nrow,ncol] = anova_stats(car::Anova(aov(
y ~ as.factor(x),
data = df.aux
), type = 2))$etasq[1]
cor_out[ncol, nrow] =  anova_stats(car::Anova(aov(
y ~ as.factor(x),
data = df.aux
), type = 2))$etasq[1]
}
if (i != j & (class(df.aux$y) != "numeric" & class(df.aux$x) == "numeric")) {
cor_out[nrow,ncol] = anova_stats(car::Anova(aov(
x ~ as.factor(y),
data = df.aux
), type = 2))$etasq[1]
cor_out[ncol, nrow] =  anova_stats(car::Anova(aov(
x ~ as.factor(y),
data = df.aux
), type = 2))$etasq[1]
}
library(rcompanion)
if (i != j & (class(df.aux$y) != "numeric" & class(df.aux$x) != "numeric")) {
cor_out[nrow,ncol] <- cramerV(table(df.aux$x, df.aux$y))
cor_out[ncol, nrow] <- cramerV(table(df.aux$x, df.aux$y))
}
}
}
return(cor_out)
}
# Usar la función sin añadir id
relationship <- corr_f(dataset %>% dplyr::select(-id, medio))
writexl::write_xlsx(as.data.frame(relationship), "famd/relationship_famd_29102025.xlsx")
#----------------------------------------------------------------------------#
# Análisis Factorial para Datos Mixtos (FAMD) para p variables explicativas  #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#----------------------------------------------------------------------------#
# Eliminar id
famd.dataset <- dataset %>% dplyr::select(-id, -medio)
##########  Implementar FAMD
library(FactoMineR)
library(factoextra)
set.seed(291025)
model <-FAMD(famd.dataset, graph = FALSE, ncp = 45)
###### Primero, examinar los valores propios y la proporción de varianza (inercia) explicada
model$eig
plot_eig = fviz_eig(model,choice='eigenvalue', geom='line') + theme_bw()
ggsave("famd/plot_eigenval_famd_29102025.png", plot = plot_eig, width = 6, height = 4, dpi = 300)
library(paran)
library(recipes)
rec <- recipe(medio ~ ., dataset) %>%
step_dummy(all_nominal_predictors(), one_hot = T)
paran.dataset <- rec %>% prep() %>% juice() %>% as.data.frame() %>% dplyr::select(-id, -medio)
cat.index <- 32:ncol(paran.dataset)
for (j in setdiff(1:ncol(paran.dataset), cat.index)) {
paran.dataset[,j] = scale(paran.dataset[,j], center = T, scale = T)
}
for (i in cat.index) {
paran.dataset[,i] = scale(paran.dataset[,i]/sqrt(nrow(paran.dataset)/sum(paran.dataset[,i])),
center = T, scale = F)
}
# Método paralelo de Horn
set.seed(123)
paran <- paran::paran(paran.dataset,
iterations = 5000)
# Librerías
library(tidyverse)
# Definir directorio
setwd("C:/Users/Portatil/Desktop/Natura/271025_Results_Med/")
# Cargar base de datos
dataset = readxl::read_excel("output/input_famd_med_29102025.xlsx")
dataset <- dataset %>%
dplyr::mutate(
across(all_of(cat_vars), ~ as.factor(.x)),    # Categóricas a factor
across(all_of(cont_vars), ~ as.numeric(.x))   # Continuas a numérico
)
#----------------------------------------------------------------------------#
# Análisis preliminar: coeficiente de correlación o razón de correlación     #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#----------------------------------------------------------------------------#
# Creación de la función para la creación de la matriz
corr_f <- function(df){
# crear output
cor_out <- matrix(NA, nrow = length(colnames(df)), ncol = length(colnames(df)))
colnames(cor_out) <- colnames(df); rownames(cor_out) <- colnames(df)
for (i in colnames(df)) {
for (j in colnames(df)) {
print(paste0(i, " vs ", j))
df.aux <- df %>% dplyr::select(i,j)
if (ncol(df.aux) ==  1) {
df.aux <- cbind(df.aux, df.aux)
colnames(df.aux) <- c("x","y")
} else{colnames(df.aux) <- c("x","y")}
nrow <- which(rownames(cor_out) == i)
ncol <- which(colnames(cor_out) == j)
if (i == j & class(df.aux$x) != "numeric") {
cor_out[nrow,ncol] = length(levels(as.factor(df.aux$x))) - 1
}
if (i == j & class(df.aux$x) == "numeric") {
cor_out[nrow,ncol] = 1
}
if (i != j & (class(df.aux$x) == "numeric" & class(df.aux$y) == "numeric")) {
cor_out[nrow,ncol] = cor(df.aux$x, df.aux$y)
cor_out[ncol, nrow] =  cor(df.aux$x, df.aux$y)
}
library(sjstats)
if (i != j & (class(df.aux$x) != "numeric" & class(df.aux$y) == "numeric")) {
cor_out[nrow,ncol] = anova_stats(car::Anova(aov(
y ~ as.factor(x),
data = df.aux
), type = 2))$etasq[1]
cor_out[ncol, nrow] =  anova_stats(car::Anova(aov(
y ~ as.factor(x),
data = df.aux
), type = 2))$etasq[1]
}
if (i != j & (class(df.aux$y) != "numeric" & class(df.aux$x) == "numeric")) {
cor_out[nrow,ncol] = anova_stats(car::Anova(aov(
x ~ as.factor(y),
data = df.aux
), type = 2))$etasq[1]
cor_out[ncol, nrow] =  anova_stats(car::Anova(aov(
x ~ as.factor(y),
data = df.aux
), type = 2))$etasq[1]
}
library(rcompanion)
if (i != j & (class(df.aux$y) != "numeric" & class(df.aux$x) != "numeric")) {
cor_out[nrow,ncol] <- cramerV(table(df.aux$x, df.aux$y))
cor_out[ncol, nrow] <- cramerV(table(df.aux$x, df.aux$y))
}
}
}
return(cor_out)
}
# Usar la función sin añadir id
relationship <- corr_f(dataset %>% dplyr::select(-id, medio))
writexl::write_xlsx(as.data.frame(relationship), "famd/relationship_famd_29102025.xlsx")
#----------------------------------------------------------------------------#
# Análisis Factorial para Datos Mixtos (FAMD) para p variables explicativas  #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#----------------------------------------------------------------------------#
# Eliminar id
famd.dataset <- dataset %>% dplyr::select(-id, -medio)
##########  Implementar FAMD
library(FactoMineR)
library(factoextra)
set.seed(291025)
model <-FAMD(famd.dataset, graph = FALSE, ncp = 45)
# model <-FAMD(famd.dataset, graph = TRUE, ncp = 20)
###### Primero, examinar los valores propios y la proporción de varianza (inercia) explicada
model$eig
plot_eig = fviz_eig(model,choice='eigenvalue', geom='line') + theme_bw()
ggsave("famd/plot_eigenval_famd_29102025.png", plot = plot_eig, width = 6, height = 4, dpi = 300)
# Computar matriz de correlación siguiendo a Páges:
# En sus entradas para variables cuantitativas, se calcula la correlación
# En sus entradas para variables cuantitativas y cualitativas, se calcula el eta2
library(paran)
library(recipes)
rec <- recipe(medio ~ ., dataset) %>%
step_dummy(all_nominal_predictors(), one_hot = T)
paran.dataset <- rec %>% prep() %>% juice() %>% as.data.frame() %>% dplyr::select(-id, -medio)
cat.index <- 32:ncol(paran.dataset)
for (j in setdiff(1:ncol(paran.dataset), cat.index)) {
paran.dataset[,j] = scale(paran.dataset[,j], center = T, scale = T)
}
for (i in cat.index) {
paran.dataset[,i] = scale(paran.dataset[,i]/sqrt(nrow(paran.dataset)/sum(paran.dataset[,i])),
center = T, scale = F)
}
# Método paralelo de Horn
set.seed(123)
paran <- paran::paran(paran.dataset,
iterations = 5000)
writexl::write_xlsx(data.frame(pc = 1:28, AdjEV = paran$AdjEv[1:28],
UnadjEV = paran$Ev[1:28], Bias = paran$Bias[1:28]),
"famd/corr_horn_paran_29102025.xlsx")
# REVISAR: LOS VALORES PROPIOS NO AJUSTADOS NO SON IDÉNTICOS A LOS OBTENIDOS DE model$eig
# PROBLEMA: ESTAMOS TOMANDO MAL LA DESCOMPOSICIÓN SVD (NO TENGO DOCUMENTACIÓN SOBRE ESA SALIDA)
###### Segundo, examinar la relación entre las variables cuantitativas
# Note: PDs are linear combinatios of the original variables.
# The factor loading of a variable describes the correlation
# between it and a given PD
# Squared factor loading correspond to squared cosine (cos2)
# This provides a measure of the proportion of variance in a variable that is
# captures by a particular PD
library(PCAmixdata)
split <- splitmix(famd.dataset)
res.pcamix <- PCAmix(X.quanti=split$X.quanti,
X.quali=split$X.quali, ndim = 28, rename.level = TRUE)
plot(res.pcamix, choice="cor")
round(res.pcamix$quanti.cor, 2)
# Nótese que el círculo de correlación en FAMD viene dado por
quantvar = round(model$quanti.var$coord[,1:28], 2)
quantvar = cbind(rownames(quantvar), quantvar)
writexl::write_xlsx(quantvar %>%
as.data.frame(),
"famd/quanti_var_cor_29102025.xlsx")
###### Tercero, squared loading plots: it allow us to visualize
# qualitative and quantitative variables in the new feature space
# "According to the authors of the package, the coordinates are to be interpreted
# as measuring “the links (signless) between variables and principal components”
library(factoextra)
p <- fviz_famd_var(model, 'var',
axes = c(1,2),
col.var = 'cos2')
ggsave("famd/plot_cos2_famd_29102025.png",
plot = p, width = 12, height = 12, dpi = 300)
fviz_add(p, model$var$coord,
col.var = 'cos2')
# Esto corresponde a:
coord_var = round(model$var$coord[,1:7],4)
coord_var = cbind(rownames(coord_var), coord_var)
writexl::write_xlsx(coord_var %>%
as.data.frame(),
"famd/coord_var_29102025.xlsx")
###### Cuarto, whereas factor loading and squared loading measure shows how well
# a given PD describes variation capture in a variable
# Contribution describes the converse: how much a variable accounts for the
# total variation captured by a PD.
library(ggpubr)
contrib = ggarrange(fviz_contrib(model, choice = "var", axes = 1),
fviz_contrib(model, choice = "var", axes = 2),
fviz_contrib(model, choice = "var", axes = 3),
fviz_contrib(model, choice = "var", axes = 4), nrow = 2, ncol = 2)
ggsave("famd/plot_contrib_famd_29102025.png",
plot = contrib, width =12, height = 12, dpi = 300)
# Quinto, Varimax rotation:
# To facilitate interpretation of the relationships between variables and PCs,
# additional rotation can be applied to PCs to result in high factor loadings
# for a few variables and low fator loadings for the rest.
# Other words: a small number of variables will become highly correlated with each PC.
# We used the most common form of rotation (Varimax rotation), a generalized form
# of which is implemented in the PCAmixdata package for mixed data
pd.rot <- PCArot(res.pcamix, dim=12,
graph=FALSE)
plot_rot = plot(pd.rot, choice="sqload",
coloring.var=TRUE, axes=c(1, 2))
ggsave("famd/plot_sqload_rotation_famd_29102025.png",
plot = plot_rot, width = 12, height = 12, dpi = 300)
# Squared loadings sobre los ejes rotados
round(pd.rot$sqload, 2)
writexl::write_xlsx(cbind(variable = rownames(pd.rot$sqload),as.data.frame(round(pd.rot$sqload,4))),
"famd/correlations_famd_29102025.xlsx")
round(pd.rot$sqload[,1][pd.rot$sqload[,1] > 0.4], 3)
round(pd.rot$sqload[,2][pd.rot$sqload[,2] > 0.4], 3)
round(pd.rot$sqload[,3][pd.rot$sqload[,3] > 0.4], 3)
round(pd.rot$sqload[,4][pd.rot$sqload[,4] > 0.4], 3)
round(pd.rot$sqload[,5][pd.rot$sqload[,5] > 0.4], 3)
round(pd.rot$sqload[,6][pd.rot$sqload[,6] > 0.4], 3)
round(pd.rot$sqload[,7][pd.rot$sqload[,7] > 0.4], 3)
round(pd.rot$sqload[,8][pd.rot$sqload[,8] > 0.4], 3)
round(pd.rot$sqload[,9][pd.rot$sqload[,9] > 0.4], 3)
round(pd.rot$sqload[,10][pd.rot$sqload[,10] > 0.4], 3)
round(pd.rot$sqload[,11][pd.rot$sqload[,11] > 0.4], 3)
round(pd.rot$sqload[,12][pd.rot$sqload[,12] > 0.4], 3)
